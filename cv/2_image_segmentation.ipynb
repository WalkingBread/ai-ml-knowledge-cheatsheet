{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects). The goal is to simplify or change the representation of an image into something that is more meaningful and easier to analyze.\n",
    "\n",
    "### Types\n",
    "\n",
    "#### Semantic Segmentation\n",
    "Every pixel in the image is assigned a class label (e.g., \"road,\" \"car,\" \"pedestrian\").\n",
    "- It does not differentiate between individual objects of the same class. All cars are just one big \"car\" blob.\n",
    "\n",
    "#### Instance Segmentation\n",
    "This goes a step further. It not only identifies the class of each pixel but also differentiates between individual instances of that class.\n",
    "- If there are five cars, each car gets its own unique mask and ID.\n",
    "\n",
    "#### Panoptic Segmentation\n",
    "It combines both:\n",
    "- Instance: Countable objects like cars and people.\n",
    "- Semantic: Amorphous regions like sky, grass, or road.\n",
    "\n",
    "### Classic Techniques\n",
    "- **Thresholding** (splitting image into foreground and background)\n",
    "- **Region-Based** (Region Growing): starts with a \"seed\" pixel and collects neighbouring pixels similar in color or texture until hits a boundary.\n",
    "- **Watershed Algorithm**: Treats the image like a topographic map. High-intensity pixels are peaks, and low-intensity are valleys. It \"floods\" the valleys with water (labels) until the different basins meet at the edges.\n",
    "- **KMean Clustering**: Effective for segmenting images based on color similarity.\n",
    "\n",
    "### Deep Learning Techniques\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lxn3ufd8_gcN0Ij2\" width=\"500\">\n",
    "\n",
    "#### U-Net\n",
    "Developed in 2015 for medical image segmentation, U-Net is a convolutional neural network with a U-shaped structure consisting of:\n",
    "- **Encoder (Contracting Path)**: Captures context using conv + pooling layers.\n",
    "- **Decoder (Expanding Path)**: Reconstructs image using transposed convolutions.\n",
    "- **Skip Connections**: Transfer fine-grained details from encoder to decoder layers.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wpPSAaAgZBq6eaCY\" width=\"400\">\n",
    "\n",
    "Imagine scanning a photo and printing it again. \n",
    "- The scanner compresses it (encoder), while the printer reconstructs the fine details (decoder). \n",
    "- Skip connections are like post-it notes helping the printer fill in the missing textures.\n",
    "\n",
    "#### Mask R-CNN\n",
    "Before the mask is drawn, the model performs standard object detection through two stages:\n",
    "- **Backbone (CNN)**: A feature extractor (usually ResNet-50 or ResNet-101) that turns the raw image into a high-level feature map.\n",
    "- **Region Proposal Network (RPN)**: This scans the feature map and proposes \"candidate\" areas (Region of Interest or RoIs) where it thinks an object might exist.\n",
    "- **The Classifier/Regressor**: For each proposal, the model predicts a class (e.g., \"Person\") and refines the bounding box coordinates.\n",
    "\n",
    "Mask R-CNN adds a third branch in parallel to the classification and bounding box branches.\n",
    "- **Output**: While the other branches output numbers (class ID and 4 coordinates), the Mask Head outputs a $m \\times m$ binary mask.\n",
    "- **Pixel-to-Pixel**: This branch is a small Fully Convolutional Network (FCN). It preserves the spatial layout of the object, allowing it to map exactly which pixels inside the bounding box belong to the object and which belong to the background.\n",
    "\n",
    "### Training Segmentation Models\n",
    "Standard \"Accuracy\" is almost never used in segmentation because of Class Imbalance (e.g., a tumor is only 1% of the image; the background is 99%).\n",
    "- **Dice Loss**: Based on the Dice Coefficient. It focuses specifically on the overlap between the predicted mask and the true mask, ignoring the \"background\" pixels.\n",
    "- **Weighted Cross-Entropy**: You tell the model to care 10x more about \"tumor\" pixels than \"empty space\" pixels.\n",
    "- **Focal Loss**: Used in DeepLab and RetinaNet. It forces the model to focus more on \"hard-to-classify\" pixels (like the thin edges of an object) rather than \"easy\" pixels (like the solid center)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
