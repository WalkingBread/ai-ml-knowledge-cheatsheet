{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a concept of analyzing and understanding the results provided by ML models. It is most often used in the context of “black-box” models, for which it is difficult to demonstrate, how did the model arrive at a specific decision.\n",
    "\n",
    "### Most common tools\n",
    "\n",
    "#### **Impurity-based Feature Importance**\n",
    "It is one of the most often used methods to get a global understanding of the tree-based model. It is applied to compute the feature_importances_ attribute in all the tree-based models in scikit-learn.\n",
    "\n",
    "It tends to over-estimate the importance of high-cardinality features (features with many unique values, like ZIP codes or IDs), even if they aren't actually that useful.\n",
    "\n",
    "#### **Permutation Feature Importance**\n",
    "Another popular technique applied to get an overall understanding of the model is Permutation FI. It is computed, by randomly permuting values of a given feature, and calculating the loss of the model’s performance caused by this distortion. To put it simply, the technique presents how much the performance of the model relies on a given feature and its quality.\n",
    "\n",
    "#### **Leave One Feature Out (LOFO)**\n",
    "This method calculates the feature importance for any model and dataset, by iteratively removing each feature from the set, and computing the model performance. In simple words, it represents how much performance would be lost if a given feature was not available.\n",
    "\n",
    "The lofo-importance package provides a python implementation of the technique.\n",
    "\n",
    "#### **SHapley Additive exPlanations (SHAP)**\n",
    "In a nutshell, SHAP computes Shapley values from game theory, which represent the contribution of a given feature towards the prediction of the model for a given sample. The explanation presents how strongly, and which way a given feature affects prediction both locally or globally.\n",
    "\n",
    "SHAP is the \"gold standard\" for explainability today. It is based on Game Theory.\n",
    "\n",
    "The Concept: The features are like players in a game, and the \"prediction\" is the total prize money. SHAP calculates how much \"credit\" each player deserves for the final score.\n",
    "\n",
    "Why it’s great: \n",
    "- It works for any model (SVM, XGBoost, Neural Nets) \n",
    "- Provides both Global and Local explanations.\n",
    "\n",
    "#### **Local Interpretable Model-agnostic Explanations (LIME)**\n",
    "LIME is a package, which focuses on explaining the prediction of a model locally. The explanation is computed, by first, generating random points around the explained sample, computing the model’s output for these points, and training a surrogate model on top of this output. Lime is especially effective for text and image-based models, however, is also applicable for tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High cardinality features are variables that have many distinct values. For example the postal code.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
