{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are capable of learning and identifying patterns directly from data without pre-defined rules. These models consist of interconnected nodes or neurons that process data.\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20251213151009129404/_neural_network.webp\" width=\"500\">\n",
    "\n",
    "- **Input Layer**: This is where the network receives its input data. Each input neuron in the layer corresponds to a feature in the input data.\n",
    "- **Hidden Layers**: These layers perform most of the computational heavy lifting. A neural network can have one or multiple hidden layers. Each layer consists of units (neurons) that transform the inputs into something that the output layer can use.\n",
    "- **Output Layer**: The final layer produces the output of the model. The format of these outputs varies depending on the specific task like classification, regression.\n",
    "\n",
    "#### Components:\n",
    "- **Neurons**: The basic units that receive inputs, each neuron is governed by a threshold and an activation function.\n",
    "- **Connections**: Links between neurons that carry information, regulated by weights and biases.\n",
    "- **Weights and Biases**: These parameters determine the strength and influence of connections.\n",
    "- **Propagation Functions**: Mechanisms that help process and transfer data across layers of neurons.\n",
    "- **Learning Rule**: The method that adjusts weights and biases over time to improve accuracy.\n",
    "\n",
    "#### Neuron:\n",
    "A single neuron:\n",
    "- Multiplies inputs by weights ($w$).\n",
    "- Adds a bias ($b$) to the result.\n",
    "- Passes the total through an Activation Function.\n",
    "\n",
    "### Learning process\n",
    "1. **Input Computation**: Data is fed into the network.\n",
    "2. **Output Generation**: Based on the current parameters, the network generates an output.\n",
    "3. **Adjusting weights and biases**: The network refines its output by adjusting weights and biases, gradually improving its performance on diverse tasks.\n",
    "\n",
    "The learning process is repeated for many iterations over the dataset.\n",
    "\n",
    "#### Forward Propagation\n",
    "Data passes through the network in the forward direction, from the input layer through the hidden layers to the output layer. This process is known as forward propagation.\n",
    "1. Linear Transformation: \n",
    "    - Each neuron in a layer receives inputs which are multiplied by the weights associated with the connections. \n",
    "    - These products are summed together and a bias is added to the sum.\n",
    "2. The result of the linear transformation is then passed through an activation function.\n",
    "\n",
    "#### Backpropagation\n",
    "After forward propagation, the network evaluates its performance using a loss function which measures the difference between the actual output and the predicted output. The goal of training is to minimize this loss.\n",
    "- **Loss Calculation**: The network calculates the loss which provides a measure of error in the predictions. The loss function could vary; common choices are mean squared error for regression tasks or cross-entropy loss for classification.\n",
    "- **Gradient Calculation**: This is to find out how much each part of the output error can be attributed to each weight and bias.\n",
    "- **Weight Update**: Once the gradients are calculated, the weights and biases are updated using an optimization algorithm like stochastic gradient descent (SGD). The weights are adjusted in the opposite direction of the gradient to minimize the loss. The size of the step taken in each update is determined by the **learning rate**.\n",
    "\n",
    "### Activation Function\n",
    "They are used to introduce non-linearity. This allows the network to learn complex patterns (like circles, waves, or faces) rather than just straight lines.\n",
    "\n",
    "Common Functions:\n",
    "- **ReLU (Rectified Linear Unit)**: Returns 0 if input is negative, and the input itself if positive. It's the \"industry standard\" because it's fast and prevents the model from \"stalling.\"\n",
    "- **Sigmoid**: Squishes values between 0 and 1. Great for binary classification.\n",
    "- **Softmax**: Used in the final layer for multi-class problems (turns outputs into probabilities that sum to 100%).\n",
    "\n",
    "#### Output Layer\n",
    "| Task | Activation Function | Why? |\n",
    "| :--- | :--- | :--- |\n",
    "| **Regression** (Predicting Price/Age) | **Linear** (None) | You need the output to be any continuous number $(-\\infty, \\infty)$. |\n",
    "| **Binary Classification** (Yes/No) | **Sigmoid** | Squishes the output between 0 and 1, representing a probability. |\n",
    "| **Multi-class Classification** | **Softmax** | Squishes outputs so they sum to 1.0 (100%) across all classes. |\n",
    "| **Multi-label Classification** | **Sigmoid** | Each class gets an independent 0-1 score (e.g., an image can be both a 'dog' AND 'indoors'). |\n",
    "\n",
    "#### Hidden Layer\n",
    "- **ReLU (Rectified Linear Unit) — The Default**\n",
    "    - Formula: $f(x) = \\max(0, x)$\n",
    "    - When to use: Use this for almost everything as your starting point.\n",
    "    - Pros: It’s computationally very fast and helps prevent the vanishing gradient problem for positive values.\n",
    "    - Cons: \"Dying ReLU\" — If neurons get knocked into the negative range, they output 0 forever and stop learning.\n",
    "- **Leaky ReLU / ELU**\n",
    "    - When to use: If you notice your network has many \"dead\" neurons (sparsity is too high) or training has stalled.\n",
    "    - How it works: It allows a tiny, non-zero gradient when the input is negative (e.g., $0.01x$).\n",
    "- **Tanh (Hyperbolic Tangent)**\n",
    "    - When to use: In RNNs (Recurrent Neural Networks) or when your data is centered around zero.\n",
    "    - Why: It outputs values between -1 and 1. Being \"zero-centered\" helps the next layer learn more efficiently than Sigmoid.\n",
    "- **Sigmoid (Avoid in hidden layers)**\n",
    "    - When to use: Almost never in modern deep hidden layers.\n",
    "    - Why: It causes the **vanishing gradient problem**. As the network gets deeper, the signal becomes so small that the early layers never update their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The \"Vanishing Gradient\" Problem\n",
    "If the network is too deep and the wrong activation functions are used, the \"signal\" sent back during backpropagation gets weaker until the weights in the early layers stop updating. The model \"stops learning.\"\n",
    "\n",
    "**The Fix**: \n",
    "- Use ReLU \n",
    "- Batch Normalization (re-centering and re-scaling the inputs to each layer).\n",
    "\n",
    "#### The \"Exploding Gradient\" Problem\n",
    "It occurs during the training of deep neural networks when the gradients (the \"signals\" used to update weights) grow exponentially as they are propagated backward through the layers.\n",
    "\n",
    "**The Fix**: \n",
    "- Gradient Clipping - setting maximum threshold for gradient\n",
    "- Batch Normalization\n",
    "- Use LSTM/GRU instead of Standard RNN - Standard RNNs are notoriously prone to exploding gradients because they reuse the exact same weight matrix over and over for every time step. LSTMs use \"gates\" to control the flow of information, which significantly dampens the effect.\n",
    "\n",
    "#### Dropout \n",
    "To prevent a Neural Network from overfitting, it is a good practice to randomly \"turn off\" a percentage of neurons during each training step. - It forces the network to be redundant and robust. It can't rely on one \"super-neuron\" to do all the work; it has to distribute the knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Define the Architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Layer 1: Input to Hidden\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        # Activation Function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Layer 2: Hidden to Output\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the path the data takes\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# 2. Initialize Model, Loss, and Optimizer\n",
    "model = SimpleNN(input_size=10, hidden_size=20, num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss() # Standard for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # The \"Engine\"\n",
    "\n",
    "# 3. Dummy Training Loop\n",
    "# In a real scenario, you'd loop over your actual dataset\n",
    "dummy_input = torch.randn(1, 10)  # One sample with 10 features\n",
    "target = torch.tensor([1])        # The true label\n",
    "\n",
    "# Forward pass\n",
    "output = model(dummy_input)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Backward pass (The magic part)\n",
    "optimizer.zero_grad() # Clear old gradients\n",
    "loss.backward()       # Calculate how to change weights\n",
    "optimizer.step()      # Update weights\n",
    "\n",
    "print(f\"Loss after one step: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "data = {\n",
    "    'feature1': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'feature2': [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    'label': [0, 0, 1, 1, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['feature1', 'feature2']].values\n",
    "y = df['label'].values\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model by specifying the loss function, optimizer \n",
    "# and metrics to evaluate during training. \n",
    "# Here binary crossentropy and adam optimizer is used.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=1, verbose=1)\n",
    "\n",
    "test_data = np.array([[0.2, 0.4]])\n",
    "prediction = model.predict(test_data)\n",
    "predicted_label = (prediction > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20251213153112518597/types_of_neural_networks.webp\" width=\"500\">\n",
    "\n",
    "| NN Type | Data Type | Key Strength |\n",
    "| :--- | :--- | :--- |\n",
    "| **ANN** | Tabular | Simple and structured patterns. |\n",
    "| **CNN** | Images | Spatial patterns and feature extraction. |\n",
    "| **RNN/LSTM** | Sequences | Temporal patterns and \"memory.\" |\n",
    "| **Transformer** | Sequences | Massive parallelization and \"attention.\" |\n",
    "| **GAN** | Generative | Creating realistic synthetic data. |\n",
    "\n",
    "#### ANN (Artificial Neural Networks)\n",
    "Also known as Multi-Layer Perceptrons (MLP), these are the \"classic\" neural networks.\n",
    "- **Structure**: They consist of an input layer, one or more hidden layers, and an output layer. Every neuron in one layer is connected to every neuron in the next (Fully Connected).\n",
    "- **Best For**: Tabular data (Excel-style data) and simple classification/regression tasks.\n",
    "- **Limitation**: They don't handle spatial or temporal relationships well. If you pass an image into an ANN, it treats pixels as independent numbers, losing the \"shape\" of the object.\n",
    "\n",
    "#### CNN (Convolutional Neural Networks)\n",
    "CNNs are the gold standard for Computer Vision.\n",
    "- **How they work**: Instead of looking at every pixel individually, they use Filters (kernels) that slide over the image to detect patterns like edges, textures, and eventually complex objects like faces.\n",
    "- **Key Layers**:\n",
    "    - Convolutional Layer: Extracts features.\n",
    "    - Pooling Layer: Downsamples the image to reduce computation and make the model \"translation invariant\" (it can recognize a cat whether it's in the top-left or bottom-right corner).\n",
    "- **Best For**: Image recognition, medical imaging, and video analysis.\n",
    "\n",
    "#### RNN (Recurrent Neural Networks)\n",
    "RNNs are designed for Sequential Data—where the order of the data points matters.\n",
    "\n",
    "- **How they work**: They have a \"loop\" (recurrence) that allows information to persist. They process inputs one by one while keeping a \"hidden state\" (memory) of what they saw previously.\n",
    "- **The Problem**: Standard RNNs have \"short-term memory.\" They struggle to remember things from the beginning of a long sentence (the Vanishing Gradient problem).\n",
    "- **Best For**: Time-series forecasting (stock prices, weather) and simple speech-to-text.\n",
    "\n",
    "#### Transformers\n",
    "- **The Innovation**: Instead of processing words one by one, Transformers look at the entire sentence at once. They use an \"Attention Mechanism\" to determine which words are most relevant to each other, regardless of how far apart they are.\n",
    "- **Best For**: Large Language Models (LLMs), chatbots, and high-end translation.\n",
    "\n",
    "#### GANs (Generative Adversarial Networks)\n",
    "GANs are used to create to generate data (images, music, art).\n",
    "- **The Logic**: Two networks are pitted against each other:\n",
    "    - **The Generator**: Tries to create a fake image.\n",
    "    - **The Discriminator**: Tries to guess if the image is real or fake.\n",
    "- **The Result**: As they compete, the Generator becomes an expert at creating incredibly realistic fake data.\n",
    "- **Best For**: Deepfakes, generating synthetic data, and image-to-image translation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
