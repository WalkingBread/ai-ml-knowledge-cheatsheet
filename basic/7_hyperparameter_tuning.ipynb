{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is the process of selecting the optimal values for a machine learning model's hyperparameters. These are typically set before the actual training process begins and control aspects of the learning process itself.\n",
    "\n",
    "Effective tuning helps the model learn better patterns, avoid overfitting or underfitting and achieve higher accuracy on unseen data.\n",
    "\n",
    "**CV - cross-validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "It trains the model using all possible combinations of specified hyperparameter values to find the best-performing setup. \n",
    "- Brute-force\n",
    "- Slow, uses a lot of computer power which makes it hard to use with big datasets or many settings.\n",
    "\n",
    "How it works:\n",
    "- Create a grid of potential values for each hyperparameter.\n",
    "- Train the model for every combination in the grid.\n",
    "- Evaluate each model using cross-validation.\n",
    "- Select the combination that gives the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': np.float64(0.006105402296585327)}\n",
      "Best score is 0.853\n"
     ]
    }
   ],
   "source": [
    "# Tuning logistic regression using GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# generate sample data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# define a range of c values (parameter) using logarithmic scale\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# GridSearchCV tries all combinations from param_grid and uses 5-fold cross-validation\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV\n",
    "It picks random combinations of hyperparameters from the given ranges instead of checking every single one.\n",
    "- In each iteration it tries a new random combination of hyperparameter values.\n",
    "- It records the modelâ€™s performance for each combination.\n",
    "- After several attempts it selects the best-performing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 5, 'min_samples_leaf': 8}\n",
      "Best score is 0.826\n"
     ]
    }
   ],
   "source": [
    "# Tuning decision tree with RandomizedSearchCV\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# define a range of values for each hyperparameter\n",
    "param_dist = {\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": randint(1, 9),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# random combinations are picked and evaluated using 5-fold cross-validation.\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization\n",
    "It treats hyperparameter tuning like a mathematical optimization problem and learns from past results to decide what to try next.\n",
    "- Build a probabilistic model (surrogate function) that predicts performance based on hyperparameters.\n",
    "- Update this model after each evaluation.\n",
    "- Use the model to choose the next best set to try.\n",
    "- Repeat until the optimal combination is found. \n",
    "\n",
    "Common surrogate models used in Bayesian optimization include:\n",
    "- Gaussian Processes\n",
    "- Random Forest Regression\n",
    "- Tree-structured Parzen Estimators (TPE)\n",
    "\n",
    "**Tip: The Optuna study can be saved to a database, which allows stopping and resuming hyperparamter tuning or running it across multiple machines at once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 11:30:39,038]\u001b[0m A new study created in memory with name: no-name-02bc37a0-dc87-458b-8176-294d21c4dcb6\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:30:55,438]\u001b[0m Trial 0 finished with value: 0.96 and parameters: {'n_estimators': 181, 'max_depth': 7}. Best is trial 0 with value: 0.96.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:30:57,348]\u001b[0m Trial 1 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 189, 'max_depth': 10}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:30:58,994]\u001b[0m Trial 2 finished with value: 0.96 and parameters: {'n_estimators': 34, 'max_depth': 18}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:00,674]\u001b[0m Trial 3 finished with value: 0.96 and parameters: {'n_estimators': 102, 'max_depth': 8}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:02,440]\u001b[0m Trial 4 finished with value: 0.9466666666666667 and parameters: {'n_estimators': 160, 'max_depth': 2}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:04,014]\u001b[0m Trial 5 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 17, 'max_depth': 9}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:04,067]\u001b[0m Trial 6 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 13, 'max_depth': 2}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:04,246]\u001b[0m Trial 7 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 105, 'max_depth': 18}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:04,485]\u001b[0m Trial 8 finished with value: 0.96 and parameters: {'n_estimators': 168, 'max_depth': 13}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:04,617]\u001b[0m Trial 9 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 83, 'max_depth': 10}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:04,834]\u001b[0m Trial 10 finished with value: 0.96 and parameters: {'n_estimators': 144, 'max_depth': 31}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:05,007]\u001b[0m Trial 11 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 73, 'max_depth': 5}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:05,181]\u001b[0m Trial 12 finished with value: 0.96 and parameters: {'n_estimators': 73, 'max_depth': 4}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:05,524]\u001b[0m Trial 13 finished with value: 0.96 and parameters: {'n_estimators': 198, 'max_depth': 4}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:05,728]\u001b[0m Trial 14 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 136, 'max_depth': 12}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:05,848]\u001b[0m Trial 15 finished with value: 0.96 and parameters: {'n_estimators': 67, 'max_depth': 32}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:06,043]\u001b[0m Trial 16 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 122, 'max_depth': 12}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:06,145]\u001b[0m Trial 17 finished with value: 0.96 and parameters: {'n_estimators': 53, 'max_depth': 5}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:06,302]\u001b[0m Trial 18 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 94, 'max_depth': 19}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:06,490]\u001b[0m Trial 19 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 128, 'max_depth': 3}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:06,763]\u001b[0m Trial 20 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 197, 'max_depth': 10}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:06,883]\u001b[0m Trial 21 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 83, 'max_depth': 6}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:06,978]\u001b[0m Trial 22 finished with value: 0.96 and parameters: {'n_estimators': 55, 'max_depth': 5}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:07,113]\u001b[0m Trial 23 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 86, 'max_depth': 3}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:07,338]\u001b[0m Trial 24 finished with value: 0.96 and parameters: {'n_estimators': 118, 'max_depth': 6}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:07,427]\u001b[0m Trial 25 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 40, 'max_depth': 14}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:07,539]\u001b[0m Trial 26 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 65, 'max_depth': 9}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:07,831]\u001b[0m Trial 27 finished with value: 0.96 and parameters: {'n_estimators': 151, 'max_depth': 7}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:08,055]\u001b[0m Trial 28 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 111, 'max_depth': 25}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:08,325]\u001b[0m Trial 29 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 182, 'max_depth': 3}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:08,483]\u001b[0m Trial 30 finished with value: 0.96 and parameters: {'n_estimators': 94, 'max_depth': 10}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:08,731]\u001b[0m Trial 31 finished with value: 0.96 and parameters: {'n_estimators': 175, 'max_depth': 12}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:08,921]\u001b[0m Trial 32 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 136, 'max_depth': 15}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:09,038]\u001b[0m Trial 33 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 75, 'max_depth': 7}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:09,125]\u001b[0m Trial 34 finished with value: 0.96 and parameters: {'n_estimators': 48, 'max_depth': 8}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:09,192]\u001b[0m Trial 35 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 28, 'max_depth': 16}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:09,416]\u001b[0m Trial 36 finished with value: 0.96 and parameters: {'n_estimators': 158, 'max_depth': 22}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:09,661]\u001b[0m Trial 37 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 187, 'max_depth': 11}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:09,805]\u001b[0m Trial 38 finished with value: 0.96 and parameters: {'n_estimators': 101, 'max_depth': 8}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:09,996]\u001b[0m Trial 39 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 135, 'max_depth': 6}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:10,246]\u001b[0m Trial 40 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 168, 'max_depth': 9}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:10,421]\u001b[0m Trial 41 finished with value: 0.96 and parameters: {'n_estimators': 93, 'max_depth': 18}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:10,584]\u001b[0m Trial 42 finished with value: 0.96 and parameters: {'n_estimators': 84, 'max_depth': 19}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:10,746]\u001b[0m Trial 43 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 95, 'max_depth': 14}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:10,924]\u001b[0m Trial 44 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 116, 'max_depth': 20}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:11,027]\u001b[0m Trial 45 finished with value: 0.96 and parameters: {'n_estimators': 61, 'max_depth': 16}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:11,151]\u001b[0m Trial 46 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 75, 'max_depth': 28}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:11,324]\u001b[0m Trial 47 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 108, 'max_depth': 11}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:11,452]\u001b[0m Trial 48 finished with value: 0.96 and parameters: {'n_estimators': 81, 'max_depth': 4}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 11:31:11,635]\u001b[0m Trial 49 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 126, 'max_depth': 5}. Best is trial 1 with value: 0.9666666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.9666666666666667\n",
      "Best Params: {'n_estimators': 189, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 1. Define the objective function\n",
    "def objective(trial):\n",
    "    # Load data inside or outside the function\n",
    "    data, target = load_iris(return_X_y=True)\n",
    "    \n",
    "    # 2. Suggest hyperparameters using the 'trial' object\n",
    "    # suggest_int for discrete values, suggest_float for continuous\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    \n",
    "    # 3. Initialize and evaluate the model\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    \n",
    "    # We use cross-validation to get a stable score\n",
    "    score = cross_val_score(clf, data, target, n_jobs=-1, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# 4. Create a study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# 5. Results\n",
    "print(f\"Best Accuracy: {study.best_value}\")\n",
    "print(f\"Best Params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation (CV)\n",
    "You should never tune hyperparameters on your test set (that's \"data leakage\"). Instead, we use K-Fold Cross-Validation.\n",
    "1. Split the training data into $K$ \"folds\" (usually 5 or 10).\n",
    "2. Train the model on $K-1$ folds and validate on the remaining fold. \n",
    "3. Repeat this $K$ times so every fold acts as the validator once. \n",
    "4. Average the scores. This gives a much more reliable estimate of how the hyperparameters will perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for most common algorithms\n",
    "\n",
    "| Algorithm | Key Hyperparameters | What they do |\n",
    "| :--- | :--- | :--- |\n",
    "| **Random Forest** | `n_estimators`, `max_depth` | Number of trees and how deep they grow. |\n",
    "| **XGBoost** | `learning_rate` ($\\eta$), `subsample` | Speed of learning and % of data used for each tree. |\n",
    "| **SVM** | `C`, `kernel`, `gamma` ($\\gamma$) | Error tolerance, boundary shape, and influence range. |\n",
    "| **KNN** | `n_neighbors`, `weights` | Number of neighbors and how much they \"count.\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Hyperparameter tuning\n",
    "- **Improved Model Performance**: Finding the optimal combination of hyperparameters can significantly boost model accuracy and robustness.\n",
    "- **Reduced Overfitting and Underfitting**: Tuning helps to prevent both overfitting and underfitting resulting in a well-balanced model.\n",
    "- **Enhanced Model Generalizability**: By selecting hyperparameters that optimize performance on validation data the model is more likely to generalize well to unseen data.\n",
    "- **Optimized Resource Utilization**: With careful tuning resources such as computation time and memory can be used more efficiently avoiding unnecessary work.\n",
    "- **Improved Model Interpretability**: Properly tuned hyperparameters can make the model simpler and easier to interpret."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
