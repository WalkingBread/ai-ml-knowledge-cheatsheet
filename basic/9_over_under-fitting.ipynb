{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-fitting / Under-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bias**: Error from erroneous assumptions (e.g., assuming a non-linear relationship is a straight line).\n",
    "- **Variance**: Error from sensitivity to small fluctuations in the training data.\n",
    "- **Irreducible Error**: The \"noise\" in the data itself that no model can ever fix.\n",
    "\n",
    "#### The Bias-Variance Tradeoff\n",
    "The goal is to find the \"Sweet Spot\" where the sum of Bias and Variance is at its lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20251209153305748438/420046946.webp\" width=\"500\">\n",
    "\n",
    "- **Underfitting**: Straight line trying to fit a curved dataset but cannot capture the data's patterns, leading to poor performance on both training and test sets.\n",
    "- **Overfitting**: A squiggly curve passing through all training points, failing to generalize performing well on training data but poorly on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "Occurs when model is too simple and does not cover all real patterns in the data.\n",
    "- It makes strong assumptions\n",
    "- Ignores the patterns\n",
    "- Variance is low because model returns similar outputs when the data changes\n",
    "\n",
    "Happens due to:\n",
    "- Model being too simple\n",
    "- Very high regularization of the data\n",
    "- Features are weak or missing\n",
    "- Not enough training\n",
    "\n",
    "**Underfitting = High Bias + Low Variance**\n",
    "\n",
    "#### How to reduce underfitting:\n",
    "- **Use a more complex model**: e.g. move from Linear Regression to a Random Forest\n",
    "- **Feature engineering**: Add more relevant input features or combine existing ones\n",
    "- **Reduce regularization**: Constraints might be too strict, preventing the model from learning\n",
    "- **Train for more epochs** (training iterations)\n",
    "- **Scale features properly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "Occurs when the model learns not just the underlying pattern, but also noise or random quirks in the training data (model memorizes training data). It performs very well on training data, but poorly on test data. \n",
    "\n",
    "Overfitting happens due to:\n",
    "- Model being too complex\n",
    "- Too many features\n",
    "- Very little data\n",
    "\n",
    "**Overfitting = Low Bias + High Variance**\n",
    "\n",
    "#### How to reduce overfitting:\n",
    "- **Collect more training data**: helps the model distinguish noise from signal\n",
    "- **Reduce model complexity**: Decrease the depth of a tree or the number of layers in a NN\n",
    "- **Regularization**: Add a penalty for complex weights (L1/L2)\n",
    "- **Apply dropout**: Randomly \"shut off\" neurons during training (Deep Learning)\n",
    "- **Early Stopping**: Stop training the moment validation error starts to rise\n",
    "- **Clean noisy data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogy:\n",
    "- **Underfitting**: A student who only reads the table of contents and fails the exam.\n",
    "- **Overfitting**: A student who memorizes specific practice questions but can't solve new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization: L1 vs. L2\n",
    "Introduces penalty for complexity. In standard machine learning, the model tries to minimize Loss (the error). Regularization changes the goal - Instead of just minimizing error, the model now tries to minimize:\n",
    "$$\\text{Total Cost} = \\text{Loss (Error)} + \\text{Penalty (Complexity)}$$\n",
    "\n",
    "#### L1 Regularization (Lasso): \n",
    "Adds the absolute value of the weights to the loss function.\n",
    "- Effect: It can push some weights to exactly zero.\n",
    "- Use case: Great for Feature Selection when you have many features and suspect only a few are actually useful.\n",
    "\n",
    "#### L2 Regularization (Ridge): \n",
    "Adds the square of the weights to the loss function.\n",
    "- Effect: It pushes weights to be very small, but never zero.\n",
    "- Use case: Good for general stability and when you want to keep all features but reduce their individual influence.\n",
    "\n",
    "**weights: the coefficients that the model learns to determine the \"importance\" or \"influence\" of each input feature.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interviewer: \"Your model has a 99% training accuracy but 70% validation accuracy.\" - they are describing Overfitting.**\n",
    "\n",
    "**Interviewer: \"I'm looking at my learning curves. Both the training loss and the validation loss have flattened out, but they are both very high. Is this an overfitting or underfitting problem, and how do I solve it?\" - That is a clear sign of Underfitting (High Bias). Because both errors are high, the model hasn't learned the basic structure of the data.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
