{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting vs Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fdDu8RbNLoUzrrlF.jpeg\" width=\"500\">\n",
    "\n",
    "Comparison: https://medium.com/@roshmitadey/bagging-v-s-boosting-be765c970fd1#a2dc\n",
    "\n",
    "Both are **Ensemble Methods** â€” meaning they combine multiple \"weak\" models to create one \"strong\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging (Bootstrap Aggregating)\n",
    "It aims to reduce Variance (overfitting).\n",
    "\n",
    "How it works: \n",
    "1. **Bootstrap**: Take your dataset and create multiple random subsets with replacement. Some rows will be repeated, and some will be left out. \n",
    "2. **Train**: Train a separate model (usually a Decision Tree) on each subset independently and in parallel. \n",
    "3. **Aggregate**: For classification, take a majority vote. For regression, take the average.\n",
    "\n",
    "The Logic: \n",
    "- Since each tree sees a slightly different version of the data, they make different mistakes. When you average them, the individual errors (noise) cancel out.\n",
    "\n",
    "Example: Random Forest Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "It aims to reduce Bias (underfitting).\n",
    "\n",
    "How it works:\n",
    "1. **Train**: Train a simple model (Tree 1) on the whole dataset. It will likely perform poorly.\n",
    "2. **Evaluate**: Identify the data points that Tree 1 got wrong (the residuals/errors).\n",
    "3. **Weight/Correct**: Train Tree 2 specifically to predict those errors.\n",
    "4. **Repeat**: Each subsequent tree tries to fix the \"mistakes\" of the previous ones.\n",
    "\n",
    "The Logic: \n",
    "- Instead of just averaging opinions, Boosting \"learns from its mistakes\" over time.\n",
    "\n",
    "Example: AdaBoost, Gradient Boosting, XGBoost, CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Which Technique\n",
    "- Use Bagging when your model is overfitting and has high variance, especially with decision trees.\n",
    "- Use Boosting when your model underfits and you want higher accuracy by learning complex patterns.\n",
    "- Use Bagging if:\n",
    "    - You have a small dataset or a lot of noise.\n",
    "    - You want a \"plug-and-play\" model that doesn't need much hyperparameter tuning.\n",
    "    - You are worried about the model memorizing the noise (overfitting).\n",
    "\n",
    "- Use Boosting if:\n",
    "    - You have plenty of data and need maximum accuracy.\n",
    "    - The relationship between features is very complex.\n",
    "    - You have the time to tune parameters like learning_rate and max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging (Random Forest)\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "# X = Feature matrix (the measurements)\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# y = Target vector (the flower species: 0, 1, or 2)\n",
    "y = data.target\n",
    "\n",
    "# Assuming X is your feature matrix and y is your labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize the model\n",
    "# n_estimators = number of trees in the forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train (Parallel process happening internally)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = rf_model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance from the trained RF model\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Visualize\n",
    "plt.barh(feature_names, importances)\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.title(\"Feature Importance - Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting (XGBoost)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Initialize the model\n",
    "# learning_rate is key here: it shrinks the contribution of each tree \n",
    "# to prevent overfitting.\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1, \n",
    "    max_depth=5,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Train (Sequential process: Tree 2 learns from Tree 1's errors)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "print(f\"XGBoost Accuracy: {accuracy_score(y_test, xgb_preds)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
