{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementations: https://medium.com/@ldeassis/decision-tree-algorithms-a-comprehensive-guide-8a15a5ddc318#9d83\n",
    "\n",
    "Algorithms: https://www.geeksforgeeks.org/machine-learning/decision-tree-algorithms/\n",
    "\n",
    "#### Decision Tree Types\n",
    "- **Classification Trees**: Used for predicting categorical outcomes like spam or not spam. These trees split the data based on features to classify data into predefined categories.\n",
    "- **Regression Trees**: Used for predicting continuous outcomes like predicting house prices. Instead of assigning categories, it provides numerical predictions based on the input features.\n",
    "\n",
    "#### Splitting Criteria\n",
    "In a Decision Tree, the process of splitting data at each node requires splitting criteria. The splitting criteria finds the best feature to split the data on. Common splitting criteria include Gini Impurity and Entropy.\n",
    "- **Gini Impurity**: This criterion measures how \"impure\" a node is. The lower the Gini Impurity the better the feature splits the data into distinct categories.\n",
    "- **Entropy**: This measures the amount of uncertainty or disorder in the data. The tree tries to reduce the entropy by splitting the data on features that provide the most information about the target variable.\n",
    "\n",
    "Gini vs. Entropy \"Why use Gini instead of Entropy?\"\n",
    "- **Gini**: Easier to calculate (no logarithms involved). It is the default in libraries like scikit-learn.\n",
    "- **Entropy**: Uses $log_2$. It is slightly more computationally expensive but can be more sensitive to small changes in probabilities.\n",
    "- **In Practice**: They yield the same results 95% of the time. Choosing one over the other rarely changes the model's performance significantly.\n",
    "\n",
    "#### Advantages of Decision Trees\n",
    "- **Easy to Understand**: Decision Trees are visual which makes it easy to follow the decision-making process.\n",
    "- **Versatility**: Can be used for both classification and regression problems.\n",
    "- **No Need for Feature Scaling**: Unlike many machine learning models, it doesnâ€™t require us to scale or normalize our data.\n",
    "- **Handles Non-linear Relationships**: It capture complex, non-linear relationships between features and outcomes effectively.\n",
    "- **Interpretability**: The tree structure is easy to interpret helps in allowing users to understand the reasoning behind each decision.\n",
    "- **Handles Missing Data**: It can handle missing values by using strategies like assigning the most common value or ignoring missing data during splits.\n",
    "\n",
    "#### Disadvantages of Decision Trees\n",
    "- **Overfitting**: They can overfit the training data if they are too deep which means they memorize the data instead of learning general patterns. This leads to poor performance on unseen data.\n",
    "- **Instability**: It can be unstable which means that small changes in the data may lead to significant differences in the tree structure and predictions.\n",
    "- **Bias towards Features with Many Categories**: It can become biased toward features with many distinct values which focuses too much on them and potentially missing other important features which can reduce prediction accuracy.\n",
    "- **Difficulty in Capturing Complex Interactions**: Decision Trees may struggle to capture complex interactions between features which helps in making them less effective for certain types of data.\n",
    "- **Computationally Expensive for Large Datasets**: For large datasets, building and pruning a Decision Tree can be computationally intensive, especially as the tree depth increases.\n",
    "\n",
    "#### Applications\n",
    "- **Loan Approval in Banking**: Banks use Decision Trees to assess whether a loan application should be approved. The decision is based on factors like credit score, income, employment status and loan history. This helps predict approval or rejection helps in enabling quick and reliable decisions.\n",
    "- **Medical Diagnosis**: In healthcare they assist in diagnosing diseases. For example, they can predict whether a patient has diabetes based on clinical data like glucose levels, BMI and blood pressure. This helps classify patients into diabetic or non-diabetic categories, supporting early diagnosis and treatment.\n",
    "- **Predicting Exam Results in Education**: Educational institutions use to predict whether a student will pass or fail based on factors like attendance, study time and past grades. This helps teachers identify at-risk students and offer targeted support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "Pruning is the technique of removing branches that have little importance, which helps to prevent overfitting. Pruning can be done pre-pruning (stopping the tree growth early) or post-pruning (removing branches from a fully grown tree). \n",
    "- Pre-pruning: Stops the tree growth early based on criteria like maximum depth or minimum number of samples required to split a node.\n",
    "- Post-pruning: Allows the tree to grow fully and then removes nodes that do not provide substantial predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
